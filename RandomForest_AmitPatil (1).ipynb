{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Fg6nbcdbFa-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('./data.csv')"
      ],
      "metadata": {
        "id": "wTHsqipSc1GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " dataset.select_dtypes(include = 'object')"
      ],
      "metadata": {
        "id": "q2XVTHAjdygz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.select_dtypes(include = ['int64', 'float64']).columns"
      ],
      "metadata": {
        "id": "iKXpdI3dd_lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.drop(columns = 'Unnamed: 32')\n",
        "dataset = pd.get_dummies(data = dataset, drop_first = True)"
      ],
      "metadata": {
        "id": "OWXrpDHne7rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(dataset['diagnosis_M'], label = 'count')"
      ],
      "metadata": {
        "id": "Bov2Z9PUfw41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_2 = dataset.drop(columns = 'diagnosis_M')"
      ],
      "metadata": {
        "id": "8DByfFKKf0Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_2.corrwith(dataset['diagnosis_M']).plot.bar(figsize = (20,10), title = ' correlation', rot = 45, grid = True)"
      ],
      "metadata": {
        "id": "wbgGQoZAf3kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr = dataset.corr()"
      ],
      "metadata": {
        "id": "LP4KOAHSgC_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,10))\n",
        "sns.heatmap(corr, annot = True)"
      ],
      "metadata": {
        "id": "407moGPTgGDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = dataset.iloc[:,1:-1].values\n",
        "y = dataset.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "DExtSwwvgX5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.tree = None\n",
        "        self.feature_order_ = []\n",
        "        #for storing the Feature indices and values at every split\n",
        "        self.split_features = []\n",
        "        self.split_values = []\n",
        "\n",
        "    def entropy(self, y):\n",
        "        _, counts = np.unique(y, return_counts=True)\n",
        "        p = counts / len(y)\n",
        "        return -np.sum(p * np.log2(p))\n",
        "\n",
        "    def information_gain(self, x, y, split_feature, split_value):\n",
        "        entropy_parent = self.entropy(y)\n",
        "        indices_left = x[:, split_feature] < split_value\n",
        "        indices_right = x[:, split_feature] >= split_value\n",
        "        entropy_left = self.entropy(y[indices_left])\n",
        "        entropy_right = self.entropy(y[indices_right])\n",
        "        size_left = len(y[indices_left])\n",
        "        size_right = len(y[indices_right])\n",
        "        entropy_children = (size_left / len(y)) * entropy_left + \\\n",
        "                           (size_right / len(y)) * entropy_right\n",
        "        information_gain = entropy_parent - entropy_children\n",
        "        return information_gain\n",
        "\n",
        "    def find_best_split(self, x, y):\n",
        "        best_feature = None\n",
        "        best_value = None\n",
        "        best_information_gain = -1\n",
        "        for feature in range(x.shape[1]):\n",
        "            values = np.unique(x[:, feature])\n",
        "            for value in values:\n",
        "                information_gain = self.information_gain(x, y, feature, value)\n",
        "                if information_gain > best_information_gain:\n",
        "                    best_feature = feature\n",
        "                    best_value = value\n",
        "                    best_information_gain = information_gain\n",
        "        return best_feature, best_value\n",
        "\n",
        "\n",
        "    def build_tree(self, x, y, depth=0):\n",
        "      if len(y) == 0:\n",
        "          return None\n",
        "\n",
        "      if depth == self.max_depth or len(y) < self.min_samples_split:\n",
        "          return np.bincount(y).argmax()\n",
        "\n",
        "      best_feature, best_value = self.find_best_split(x, y)\n",
        "\n",
        "      # Keep track of the feature order\n",
        "      self.feature_order_.append(best_feature)\n",
        "\n",
        "\n",
        "      indices_left = x[:, best_feature] < best_value\n",
        "      indices_right = x[:, best_feature] >= best_value\n",
        "\n",
        "      if np.all(indices_left) or np.all(indices_right):\n",
        "          return np.bincount(y).argmax()\n",
        "      self.split_features.append(best_feature)\n",
        "      self.split_values.append(best_value)\n",
        "\n",
        "      left_subtree = self.build_tree(x[indices_left], y[indices_left], depth+1)\n",
        "      right_subtree = self.build_tree(x[indices_right], y[indices_right], depth+1)\n",
        "\n",
        "      return (best_feature, best_value, left_subtree, right_subtree)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        self.tree = self.build_tree(x, y)\n",
        "\n",
        "    def predict(self, x):\n",
        "        y_pred = np.zeros(x.shape[0])\n",
        "        for i, sample in enumerate(x):\n",
        "            node = self.tree\n",
        "            while isinstance(node, tuple):\n",
        "                feature, value, left_subtree, right_subtree = node\n",
        "                if sample[feature] < value:\n",
        "                    node = left_subtree\n",
        "                else:\n",
        "                    node = right_subtree\n",
        "            y_pred[i] = node\n",
        "        return y_pred\n"
      ],
      "metadata": {
        "id": "dpSP_YntuET8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size = 0.25, random_state = 45)\n"
      ],
      "metadata": {
        "id": "NvKJjMy0hqYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.transform(x_test)"
      ],
      "metadata": {
        "id": "uU7PV5jMhauW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "dt = DecisionTree(max_depth=2, min_samples_split=2)\n",
        "dt.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "print(\"Order of feature selection:\", dt.feature_order_)\n",
        "\n",
        "\n",
        "split_features = dt.split_features\n",
        "split_values = dt.split_values\n",
        "for i in range(len(split_features)):\n",
        "    print(\"Node {}: split feature={}, split value={}\".format(i+1, split_features[i], split_values[i]))\n",
        "\n",
        "\n",
        "y_pred = dt.predict(x_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "kC-__IHMgK_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1-score: {f1}')"
      ],
      "metadata": {
        "id": "Hlfpb_C27hWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest class for Breast Cancer Detection Dataset\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class RandomForest():\n",
        "    def __init__(self, n_estimators=100, max_depth=2, min_samples_split=2):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        for i in range(self.n_estimators):\n",
        "            indices = np.random.choice(x.shape[0], size=x.shape[0], replace=True)\n",
        "            X_subset = x[indices]\n",
        "            y_subset = y[indices]\n",
        "\n",
        "            if len(X_subset) > 0 and len(y_subset) > 0:\n",
        "              tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
        "              tree.fit(X_subset, y_subset)\n",
        "              self.trees.append(tree)\n",
        "\n",
        "    def predict(self, x):\n",
        "        y_preds = np.zeros((x.shape[0], len(self.trees)))\n",
        "        for i, tree in enumerate(self.trees):\n",
        "            y_preds[:, i] = tree.predict(x)\n",
        "\n",
        "        y_pred = np.zeros(x.shape[0])\n",
        "        for i in range(x.shape[0]):\n",
        "            counts = np.bincount(y_preds[i, :].astype('int'))\n",
        "            y_pred[i] = np.argmax(counts)\n",
        "\n",
        "        return y_pred\n",
        "    def get_params(self, deep=True):\n",
        "      return {'n_estimators': self.n_estimators, 'max_depth': self.max_depth, 'min_samples_split': self.min_samples_split}\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for param, value in params.items():\n",
        "            setattr(self, param, value)\n",
        "        return self\n",
        "    def score(self, x, y):\n",
        "        y_pred = self.predict(x)\n",
        "        return accuracy_score(y, y_pred)"
      ],
      "metadata": {
        "id": "XiupsY3z_piD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "kfold = KFold(n_splits=10)\n",
        "\n",
        "accuracy_scores = []\n",
        "f1_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "\n",
        "\n",
        "for train_index, test_index in kfold.split(x):\n",
        "\n",
        "    x_train, x_test = x[train_index], x[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    rf = RandomForest(n_estimators=100, max_depth=2, min_samples_split=2)\n",
        "    rf.fit(x_train, y_train)\n",
        "\n",
        "    y_pred = rf.predict(x_test)\n",
        "\n",
        "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "    f1_scores.append(f1_score(y_test, y_pred))\n",
        "    precision_scores.append(precision_score(y_test, y_pred))\n",
        "    recall_scores.append(recall_score(y_test, y_pred))\n",
        "\n",
        "mean_accuracy = np.mean(accuracy_scores)\n",
        "std_accuracy = np.std(accuracy_scores)\n",
        "mean_f1 = np.mean(f1_scores)\n",
        "std_f1 = np.std(f1_scores)\n",
        "mean_precision = np.mean(precision_scores)\n",
        "std_precision = np.std(precision_scores)\n",
        "mean_recall = np.mean(recall_scores)\n",
        "std_recall = np.std(recall_scores)\n",
        "\n",
        "print('Mean accuracy:', mean_accuracy)\n",
        "print('SD of accuracy:', std_accuracy)\n",
        "print('Mean F1 score:', mean_f1)\n",
        "print('SD of F1 score:', std_f1)\n",
        "print('Mean precision:', mean_precision)\n",
        "print('SD of precision:', std_precision)\n",
        "print('Mean recall:', mean_recall)\n",
        "print('SD of recall:', std_recall)"
      ],
      "metadata": {
        "id": "_I94fSixjRc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "rf = RandomForest(n_estimators=100, max_depth=20, min_samples_split=2)\n",
        "\n",
        "rf.fit(x_train, y_train)\n",
        "\n",
        "y_pred = rf.predict(x_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "s2SMv0AbCcej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500],\n",
        "    'max_depth': [5, 10, 15, 20, 25],\n",
        "    'min_samples_split': [2, 4, 6, 8, 10],\n",
        "}\n",
        "\n",
        "\n",
        "rf = RandomForest()\n",
        "\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=10)\n",
        "\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "jBKAAPcKeBKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1 score:', f1)"
      ],
      "metadata": {
        "id": "AS0vUbLzEGjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "rf = RandomForest(n_estimators=100, max_depth=2, min_samples_split=2)\n",
        "scores = cross_val_score(rf, x, y, cv=10)\n"
      ],
      "metadata": {
        "id": "Zu5pIDjCcjlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [5, 10, 15, 20, 25],\n",
        "    'min_samples_split': [2, 4, 6, 8, 10]\n",
        "}\n",
        "\n",
        "cv = 10\n",
        "\n",
        "\n",
        "scoring_matrix = ['accuracy', 'f1', 'precision', 'recall']\n",
        "\n",
        "for hyperparameter_name, hyperparameter_values in hyperparameters.items():\n",
        "\n",
        "    fig, axs = plt.subplots(1, len(scoring_matrix), figsize=(20, 5), gridspec_kw={'wspace': 0.3})\n",
        "    fig.suptitle(f\"{hyperparameter_name.capitalize()}\")\n",
        "\n",
        "    for i, metric in enumerate(scoring_matrix):\n",
        "\n",
        "        mean_scores = []\n",
        "        std_scores = []\n",
        "\n",
        "        for value in hyperparameter_values:\n",
        "            if hyperparameter_name == 'n_estimators':\n",
        "                rf = RandomForest(n_estimators=value)\n",
        "            elif hyperparameter_name == 'max_depth':\n",
        "                rf = RandomForest(max_depth=value)\n",
        "            elif hyperparameter_name == 'min_samples_split':\n",
        "                rf = RandomForest(min_samples_split=value)\n",
        "            elif hyperparameter_name == 'min_samples_leaf':\n",
        "                rf = RandomForest(min_samples_leaf=value)\n",
        "            elif hyperparameter_name == 'criterion':\n",
        "                rf = RandomForest(criterion=value)\n",
        "\n",
        "            scores = cross_val_score(rf, x, y, cv=cv, scoring=metric)\n",
        "            mean_scores.append(np.mean(scores))\n",
        "            std_scores.append(np.std(scores))\n",
        "\n",
        "        axs[i].errorbar(hyperparameter_values, mean_scores, yerr=std_scores, fmt='o-', capsize=5)\n",
        "        axs[i].set_xlabel(hyperparameter_name.capitalize())\n",
        "        axs[i].set_ylabel(metric)\n",
        "        axs[i].set_title(f\"{metric} vs. {hyperparameter_name.capitalize()}\")\n",
        "\n",
        "    plt.savefig(f'hyperparameter_tuning_feature{hyperparameter_name}.png')\n",
        "#     plt.show()\n",
        "\n",
        "plt.savefig('hyperparameter_tuning_feature.png', dpi=100)\n"
      ],
      "metadata": {
        "id": "EKXIpP1Tc_0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest class for new artificial dataset\n",
        "class RandomForest_new():\n",
        "    def __init__(self, n_estimators=100, max_depth=2, min_samples_split=2):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        for i in range(self.n_estimators):\n",
        "            indices = np.random.choice(x.shape[0], size=x.shape[0], replace=True)\n",
        "            X_subset = x[indices]\n",
        "            y_subset = y[indices]\n",
        "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
        "            tree.fit(X_subset, y_subset)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, x):\n",
        "        tree_preds = []\n",
        "        for i, tree in enumerate(self.trees):\n",
        "            tree_preds.append(tree.predict(x.reshape(1, -1))[0])\n",
        "        return tree_preds\n"
      ],
      "metadata": {
        "id": "5-CKuvaj4aqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing artificial dataset\n",
        "dataset_new = pd.read_csv('./my_dataset.csv')"
      ],
      "metadata": {
        "id": "dGUAz3k_6wGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = dataset_new.iloc[:, :-1].values\n",
        "y = dataset_new.iloc[:, -1].values"
      ],
      "metadata": {
        "id": "nxEgKjzJ7eMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "train_indices = np.random.choice(X.shape[0], size=12, replace=False)\n",
        "\n",
        "X_train, y_train = X[train_indices], y[train_indices]\n",
        "\n",
        "X_test, y_test = np.array([[4, 4]]), np.array([2])"
      ],
      "metadata": {
        "id": "OnjMk-0Q7f7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForest_new(n_estimators=20, max_depth=2, min_samples_split=2)\n",
        "rf.fit(X_train, y_train)\n",
        "x = X_test[0]\n",
        "tree_preds = rf.predict(x)\n",
        "for i, pred in enumerate(tree_preds):\n",
        "    print(f\"Tree {i+1} prediction: {pred}\")"
      ],
      "metadata": {
        "id": "i3id1-Ct8nqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "rf = RandomForest_new(n_estimators=20, max_depth=2, min_samples_split=2)\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "0SvXBmPzD34f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "GmS1EDWMENi7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}